{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30bcf562",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6654e14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "#Tokenizer from scratch on vocabulary of corpus\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# Decoder\n",
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaForMaskedLM # RobertaLM for learning\n",
    "from transformers import RobertaTokenizerFast # After training tokenizern we will wrap it so it can be used by Roberta model\n",
    "\n",
    "\n",
    "#Training\n",
    "# When using previous version of the library you need the following two lines\n",
    "from transformers import Seq2SeqTrainer\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2f1a73",
   "metadata": {},
   "source": [
    "# Parameters for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a0af58b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 20   # input batch size for training (default: 64)\n",
    "VALID_BATCH_SIZE = 5   # input batch size for testing (default: 1000)\n",
    "VAL_EPOCHS = 1 \n",
    "LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n",
    "SEED = 42               # random seed (default: 42)\n",
    "MAX_LEN = 128           # Max length for product description\n",
    "SUMMARY_LEN = 20         # Max length for product names\n",
    "\n",
    "TRAIN_EPOCHS = 2       # number of epochs to train (default: 10)\n",
    "WEIGHT_DECAY = 0.01\n",
    "SEED = 42               # random seed (default: 42)\n",
    "MAX_LEN = 128\n",
    "SUMMARY_LEN = 20   # Maximum length of caption generated by the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fe6145",
   "metadata": {},
   "source": [
    "# Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a581f10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'C:\\Users\\kalpe\\Dropbox\\PC\\Documents\\Great lakes\\ML project')\n",
    "import json\n",
    "\n",
    "\n",
    "with open('data.json', 'r') as openfile:\n",
    "\n",
    "    json_object = json.load(openfile)\n",
    "\n",
    "images_caption_dict = dict(json_object)\n",
    "\n",
    "\n",
    "images_path = 'C:/Users/kalpe/Documents/Great lakes/ML project/Flickr8k_Dataset/Flicker8k_Dataset/'\n",
    "images = list(images_caption_dict.keys())\n",
    "\n",
    "for image_path in images:\n",
    "    if image_path.endswith('jpg'):\n",
    "        new = images_path + image_path.split('/')[-1]\n",
    "        images_caption_dict[new] = images_caption_dict.pop(image_path)\n",
    "    else:\n",
    "        images_caption_dict.pop(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "203d74b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "captions = []\n",
    "images = []\n",
    "for image in list(images_caption_dict.keys()):\n",
    "    caption = images_caption_dict[image]\n",
    "#     captions.append(('.'.join([ sent.rstrip() for sent in ('.'.join(caption)).split('<e>.<s>')]))\\\n",
    "#                             .replace('<s> ','').replace('  <e>','.'))\n",
    "    for capt in caption:\n",
    "        captions.append(capt.replace('<s> ','').replace('  <e>','').strip())\n",
    "        images.append(image)\n",
    "        \n",
    "df['images'] = images\n",
    "df['captions'] = captions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674ab7d3",
   "metadata": {},
   "source": [
    "# ROBERTA\n",
    "### Training the Decoder Model for Language Understanding and build Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b12a0c9",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "#### Converting captions in to .txt file for training of the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "e956c1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store values in a dataframe column (Series object) to files, one file per record\n",
    "os.mkdir(\"./text_split\")\n",
    "def column_to_files(column, prefix, txt_files_dir = \"./text_split\"):\n",
    "    # The prefix is a unique ID to avoid to overwrite a text file\n",
    "    i=prefix\n",
    "    #For every value in the df, with just one column\n",
    "    for row in column.to_list():\n",
    "      # Create the filename using the prefix ID\n",
    "        file_name = os.path.join(txt_files_dir, str(i)+'.txt')\n",
    "        try:\n",
    "            # Create the file and write the column text to it\n",
    "            f = open(file_name, 'wb')\n",
    "            f.write(row.encode('utf-8'))\n",
    "            f.close()\n",
    "        except Exception as e:  #catch exceptions(for eg. empty rows)\n",
    "            print(row, e) \n",
    "        i+=1\n",
    "    # Return the last ID\n",
    "    return i\n",
    "\n",
    "data = df[\"captions\"]\n",
    "# Removing the end of line character \\n\n",
    "data = data.replace(\"\\n\",\" \")\n",
    "# Set the ID to 0\n",
    "prefix=0\n",
    "# Create a file for every description value\n",
    "prefix = column_to_files(data, prefix)\n",
    "# Print the last ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d55eaff",
   "metadata": {},
   "source": [
    "#### Training tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "23b01bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "paths = [str(x) for x in Path(\".\").glob(\"text_split/*.txt\")]\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer(lowercase=True)\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=10000, min_frequency=2,\n",
    "                show_progress=True,\n",
    "                special_tokens=[\n",
    "                                \"<s>\",\n",
    "                                \"<pad>\",\n",
    "                                \"<e>\",\n",
    "                                \"<unk>\",\n",
    "                                \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2599851c",
   "metadata": {},
   "source": [
    "#### Save Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "463597cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Byte_tokenizer\\\\vocab.json', 'Byte_tokenizer\\\\merges.txt']"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "os.mkdir('Byte_tokenizer')\n",
    "tokenizer.save_model('Byte_tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c98310",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "#### Intialization & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "617c6d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RobertaConfig(\n",
    "    vocab_size=10000,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "\n",
    "print('Num parameters: ',model.num_parameters())\n",
    "\n",
    "# Create the tokenizer from a trained one\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('Byte_tokenizer', max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "c8ae1ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        # or use the RobertaTokenizer from `transformers` directly.\n",
    "\n",
    "        self.examples = []\n",
    "        \n",
    "        for example in df.values:\n",
    "            x=tokenizer.encode_plus(example, max_length = MAX_LEN, truncation=True, padding=True)\n",
    "            self.examples += [x.input_ids]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # Weâ€™ll pad at the batch level.\n",
    "        return torch.tensor(self.examples[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "8289594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the train and evaluation dataset\n",
    "train_dataset = CustomDataset(df['captions'][:38000], tokenizer)\n",
    "eval_dataset = CustomDataset(df['captions'][38000:], tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20893ec7",
   "metadata": {},
   "source": [
    "#### Batching Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "a4f6f890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Define the Data Collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18e9ba3",
   "metadata": {},
   "source": [
    "## Training the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "ae2f832b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "model_folder = \"RobertaMLM\"\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_folder,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    num_train_epochs=TRAIN_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=VALID_BATCH_SIZE,\n",
    "    save_steps=8192,\n",
    "    #eval_steps=4096,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "# Create the trainer for our model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    #prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "cc692d01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 38000\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 35625\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35625' max='35625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35625/35625 13:33:38, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.151400</td>\n",
       "      <td>3.852298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.435300</td>\n",
       "      <td>3.333788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.171100</td>\n",
       "      <td>3.140997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.919000</td>\n",
       "      <td>2.931470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.826800</td>\n",
       "      <td>2.734056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.626200</td>\n",
       "      <td>2.689468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.511100</td>\n",
       "      <td>2.565856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.419900</td>\n",
       "      <td>2.635178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.368700</td>\n",
       "      <td>2.596702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.324700</td>\n",
       "      <td>2.430679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.231800</td>\n",
       "      <td>2.309738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.175700</td>\n",
       "      <td>2.357576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.104100</td>\n",
       "      <td>2.333789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.061800</td>\n",
       "      <td>2.235854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.035100</td>\n",
       "      <td>2.218165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2455\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2455\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2455\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to RobertaMLM\\checkpoint-8192\n",
      "Configuration saved in RobertaMLM\\checkpoint-8192\\config.json\n",
      "Model weights saved in RobertaMLM\\checkpoint-8192\\pytorch_model.bin\n",
      "Deleting older checkpoint [RobertaMLM\\checkpoint-6500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2455\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2455\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2455\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to RobertaMLM\\checkpoint-16384\n",
      "Configuration saved in RobertaMLM\\checkpoint-16384\\config.json\n",
      "Model weights saved in RobertaMLM\\checkpoint-16384\\pytorch_model.bin\n",
      "Deleting older checkpoint [RobertaMLM\\checkpoint-8192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2455\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2455\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2455\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2455\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to RobertaMLM\\checkpoint-24576\n",
      "Configuration saved in RobertaMLM\\checkpoint-24576\\config.json\n",
      "Model weights saved in RobertaMLM\\checkpoint-24576\\pytorch_model.bin\n",
      "Deleting older checkpoint [RobertaMLM\\checkpoint-16384] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2455\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2455\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2455\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to RobertaMLM\\checkpoint-32768\n",
      "Configuration saved in RobertaMLM\\checkpoint-32768\\config.json\n",
      "Model weights saved in RobertaMLM\\checkpoint-32768\\pytorch_model.bin\n",
      "Deleting older checkpoint [RobertaMLM\\checkpoint-24576] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2455\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2455\n",
      "  Batch size = 4\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=35625, training_loss=2.685712823807566, metrics={'train_runtime': 48820.4074, 'train_samples_per_second': 11.675, 'train_steps_per_second': 0.73, 'total_flos': 3127241154338304.0, 'train_loss': 2.685712823807566, 'epoch': 15.0})"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9fe382",
   "metadata": {},
   "source": [
    "#### Check Perplexity score of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d01728b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fed68da",
   "metadata": {},
   "source": [
    "### Saving tokenizer & Model to use in Encoder Decoder architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "ba4ad34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Byte_tokenizer\\tokenizer_config.json\n",
      "Special tokens file saved in Byte_tokenizer\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Byte_tokenizer\\\\tokenizer_config.json',\n",
       " 'Byte_tokenizer\\\\special_tokens_map.json',\n",
       " 'Byte_tokenizer\\\\vocab.json',\n",
       " 'Byte_tokenizer\\\\merges.txt',\n",
       " 'Byte_tokenizer\\\\added_tokens.json',\n",
       " 'Byte_tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('Byte_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "62f26ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to RobertaMLM\n",
      "Configuration saved in RobertaMLM\\config.json\n",
      "Model weights saved in RobertaMLM\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(model_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a830a53d",
   "metadata": {},
   "source": [
    "# Evaluating Decoder(ROBERTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa2efa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model= r'RobertaMLM',\n",
    "    tokenizer= 'Byte_tokenizer'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b3853b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'a girl going into a brick building',\n",
       "  'score': 0.2611638009548187,\n",
       "  'token': 1043,\n",
       "  'token_str': ' brick'},\n",
       " {'sequence': 'a girl going into a white building',\n",
       "  'score': 0.10348810255527496,\n",
       "  'token': 340,\n",
       "  'token_str': ' white'},\n",
       " {'sequence': 'a girl going into a red building',\n",
       "  'score': 0.0433187335729599,\n",
       "  'token': 377,\n",
       "  'token_str': ' red'},\n",
       " {'sequence': 'a girl going into a large building',\n",
       "  'score': 0.03820646554231644,\n",
       "  'token': 491,\n",
       "  'token_str': ' large'},\n",
       " {'sequence': 'a girl going into a blue building',\n",
       "  'score': 0.03518558293581009,\n",
       "  'token': 402,\n",
       "  'token_str': ' blue'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"a girl going into a <mask> building\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8099aff",
   "metadata": {},
   "source": [
    "## This Roberta Model will be used as Decoder in Our Image Captioning model and will be connnected to ViT Encoder model using cross attention heads."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
